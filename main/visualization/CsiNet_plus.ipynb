{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'espnet_model.pdf.pdf'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import OrderedDict\n",
    "# from CsiNet_plus import *\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "import tensorwatch as tw\n",
    "\n",
    "\n",
    "NUM_FEEDBACK_BITS = 512 #pytorch版本一定要有这个参数\n",
    "\n",
    "\n",
    "# This part implement the quantization and dequantization operations.\n",
    "# The output of the encoder must be the bitstream.\n",
    "def Num2Bit(Num, B):\n",
    "    Num_ = Num.type(torch.uint8)\n",
    "\n",
    "    def integer2bit(integer, num_bits=B * 2):\n",
    "        dtype = integer.type()\n",
    "        exponent_bits = -torch.arange(-(num_bits - 1), 1).type(dtype)\n",
    "        exponent_bits = exponent_bits.repeat(integer.shape + (1,))\n",
    "        out = integer.unsqueeze(-1) // 2 ** exponent_bits\n",
    "        return (out - (out % 1)) % 2\n",
    "\n",
    "    bit = integer2bit(Num_)\n",
    "    bit = (bit[:, :, B:]).reshape(-1, Num_.shape[1] * B)\n",
    "    return bit.type(torch.float32)\n",
    "\n",
    "\n",
    "def Bit2Num(Bit, B):\n",
    "    Bit_ = Bit.type(torch.float32)\n",
    "    Bit_ = torch.reshape(Bit_, [-1, int(Bit_.shape[1] / B), B])\n",
    "    num = torch.zeros(Bit_[:, :, 1].shape)\n",
    "    for i in range(B):\n",
    "        num = num + Bit_[:, :, i] * 2 ** (B - 1 - i)\n",
    "    return num\n",
    "\n",
    "\n",
    "class Quantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = torch.round(x * step - 0.5)\n",
    "        out = Num2Bit(out, B)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of constant arguments to forward must be None.\n",
    "        # Gradient of a number is the sum of its four bits.\n",
    "        b, _ = grad_output.shape\n",
    "        grad_num = torch.sum(grad_output.reshape(b, -1, ctx.constant), dim=2)\n",
    "        return grad_num, None\n",
    "\n",
    "\n",
    "class Dequantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = Bit2Num(x, B)\n",
    "        out = (out + 0.5) / step\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        # repeat the gradient of a Num for four time.\n",
    "        #b, c = grad_output.shape\n",
    "        #grad_bit = grad_output.repeat(1, 1, ctx.constant) \n",
    "        #return torch.reshape(grad_bit, (-1, c * ctx.constant)), None\n",
    "        grad_bit = grad_output.repeat_interleave(ctx.constant, dim=1)\n",
    "        return grad_bit, None\n",
    "\n",
    "\n",
    "class QuantizationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, B):\n",
    "        super(QuantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Quantization.apply(x, self.B)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DequantizationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, B):\n",
    "        super(DequantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Dequantization.apply(x, self.B)\n",
    "        return out\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=True)\n",
    "\n",
    "\n",
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, groups=1):\n",
    "        if not isinstance(kernel_size, int):\n",
    "            padding = [(i - 1) // 2 for i in kernel_size]\n",
    "        else:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        super(ConvBN, self).__init__(OrderedDict([\n",
    "            ('conv', nn.Conv2d(in_planes, out_planes, kernel_size, stride,\n",
    "                               padding=padding, groups=groups, bias=False)),\n",
    "            ('bn', nn.BatchNorm2d(out_planes))\n",
    "        ]))\n",
    "\n",
    "class CRBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRBlock, self).__init__()\n",
    "        self.path1 = nn.Sequential(OrderedDict([\n",
    "            ('conv3x3', ConvBN(32, 32, 3)),\n",
    "            ('relu1', nn.LeakyReLU(negative_slope=0.3, inplace=False)),\n",
    "            ('conv1x9', ConvBN(32, 32, [1, 9])),\n",
    "            ('relu2', nn.LeakyReLU(negative_slope=0.3, inplace=False)),\n",
    "            ('conv9x1', ConvBN(32, 32, [9, 1])),\n",
    "        ]))\n",
    "        self.path2 = nn.Sequential(OrderedDict([\n",
    "            ('conv1x5', ConvBN(32, 32, [1, 5])),\n",
    "            ('relu', nn.LeakyReLU(negative_slope=0.3, inplace=False)),\n",
    "            ('conv5x1', ConvBN(32, 32, [5, 1])),\n",
    "        ]))\n",
    "        self.conv1x1 = ConvBN(32 * 2, 32, 1)\n",
    "        self.identity = nn.Identity()\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.3, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "        \n",
    "        out1 = self.path1(x)\n",
    "        out2 = self.path2(x)\n",
    "        out = torch.cat((out1, out2), dim=1)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1x1(out)\n",
    "        \n",
    "        out = self.relu(out + identity)\n",
    "        return out\n",
    "    \n",
    "class CR_encoder(nn.Module):\n",
    "    def __init__(self,ch):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.encoder1 = nn.Sequential(OrderedDict([\n",
    "            (\"conv1x9_bn\", ConvBN(ch, ch, [1, 9])),\n",
    "            (\"relu\", nn.LeakyReLU(negative_slope=0.3, inplace=False)),\n",
    "            (\"conv9x1_bn\", ConvBN(ch, ch, [9, 1])),\n",
    "        ]))\n",
    "        self.encoder2 = ConvBN(ch,ch, 3)\n",
    "        self.encoder3 = nn.Sequential(OrderedDict([\n",
    "            (\"conv1x5_bn\", ConvBN(ch, ch, [1, 5])),\n",
    "            (\"relu\", nn.LeakyReLU(negative_slope=0.3, inplace=False)),\n",
    "            (\"conv5x1_bn\", ConvBN(ch, ch, [5, 1])),\n",
    "        ]))\n",
    "        self.encoder_conv = nn.Sequential(OrderedDict([\n",
    "            (\"relu1\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            (\"conv1x1_bn\", ConvBN(ch*3, ch, 1)),\n",
    "            #(\"relu\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "        ]))\n",
    "    def forward(self, x):\n",
    "        encode1 = self.encoder1(x)\n",
    "        encode2 = self.encoder2(x)\n",
    "        encode3 = self.encoder3(x)\n",
    "        out = torch.cat((encode1, encode2,encode3), dim=1)\n",
    "        #out = self.encoder_conv(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "class ResBlock_CRNET(nn.Module):\n",
    "    def __init__(self,ch,nblocks=1,shortcut=True):\n",
    "        super().__init__()\n",
    "        self.shortcut= shortcut\n",
    "        self.module_list = nn.ModuleList()\n",
    "        for i in range(nblocks):\n",
    "            resblock = nn.ModuleList()\n",
    "            resblock.append(ConvBN(ch,ch,1))\n",
    "            resblock.append(nn.LeakyReLU(negative_slope=0.3, inplace=False))\n",
    "            resblock.append(CR_encoder(ch))\n",
    "            resblock.append(nn.LeakyReLU(negative_slope=0.3, inplace=False))\n",
    "            resblock.append(ConvBN(ch*3,ch,1))\n",
    "            resblock.append(nn.LeakyReLU(negative_slope=0.3, inplace=False))\n",
    "            self.module_list.append(resblock)\n",
    "        self.identity = nn.Identity()   \n",
    "    def forward(self,x):\n",
    "        for module in self.module_list:\n",
    "            h = x\n",
    "            identity = self.identity(x)\n",
    "            for res in module:\n",
    "                h = res(h)\n",
    "            out = identity+h if self.shortcut else h \n",
    "        return out    \n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,ch,nblocks=1,shortcut=True):\n",
    "        super().__init__()\n",
    "        self.shortcut= shortcut\n",
    "        self.module_list = nn.ModuleList()\n",
    "        self.identity = nn.Identity() \n",
    "        for i in range(nblocks):\n",
    "            resblock = nn.ModuleList()\n",
    "            resblock.append(ConvBN(ch,ch,1))\n",
    "            resblock.append(nn.LeakyReLU(negative_slope=0.3, inplace=True))\n",
    "            resblock.append(ConvBN(ch,ch,3))\n",
    "            resblock.append(nn.LeakyReLU(negative_slope=0.3, inplace=True))\n",
    "            resblock.append(ConvBN(ch,ch,1))\n",
    "            self.module_list.append(resblock)\n",
    "    def forward(self,x):\n",
    "        for module in self.module_list:\n",
    "            h = x \n",
    "            identity = self.identity(x)\n",
    "            for res in module:\n",
    "                h = res(h)\n",
    "            x = identity+h if self.shortcut else h \n",
    "        return x     \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    B = 4\n",
    "\n",
    "    def __init__(self, feedback_bits, quantization=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            (\"conv3x3_bn\", ConvBN(2, 32, 3)),\n",
    "            (\"relu1\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            (\"ResBlock_CRNET_1\", ResBlock_CRNET(32)),\n",
    "            (\"ResBlock_CRNET_2\", ResBlock_CRNET(32)),\n",
    "            (\"ResBlock_CRNET_3\", ResBlock_CRNET(32)),\n",
    "            (\"ResBlock_CRNET_4\", ResBlock_CRNET(32)),\n",
    "        ]))\n",
    "        \n",
    "        \n",
    "        self.encoder_conv = nn.Sequential(OrderedDict([\n",
    "            (\"relu1\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            (\"conv1x1_bn\", ConvBN(32, 2, 1)),\n",
    "            (\"relu2\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "        ]))\n",
    "\n",
    "        self.fc = nn.Linear(768, int(feedback_bits / self.B))\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.quantize = QuantizationLayer(self.B)\n",
    "        self.quantization = quantization \n",
    "        # if self.quantization:\n",
    "        #     for p in self.parameters():\n",
    "        #         p.requires_grad=False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,3,1,2)\n",
    "        encode = self.encoder(x)\n",
    "        out = self.encoder_conv(encode)\n",
    "        out = out.reshape(-1, 768)\n",
    "        out = self.fc(out)\n",
    "        out = self.sig(out)\n",
    "        if self.quantization:\n",
    "            out = self.quantize(out)\n",
    "        else:\n",
    "            out = out\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    B = 4\n",
    "\n",
    "    def __init__(self, feedback_bits, quantization=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.feedback_bits = feedback_bits\n",
    "        self.dequantize = DequantizationLayer(self.B)\n",
    "        self.fc = nn.Linear(int(feedback_bits / self.B), 768)\n",
    "        decoder = OrderedDict([\n",
    "            (\"conv5x5_bn\", ConvBN(2, 32, 5)),\n",
    "            (\"relu\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            (\"CRBlock1\", CRBlock()),\n",
    "            (\"CRBlock2\", CRBlock()),\n",
    "            (\"CRBlock3\", CRBlock()),\n",
    "            (\"CRBlock4\", CRBlock()),\n",
    "        ])\n",
    "        self.decoder_feature = nn.Sequential(decoder)\n",
    "        self.out_cov = conv3x3(32, 2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.quantization = quantization        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.quantization:\n",
    "            out = self.dequantize(x)\n",
    "        else:\n",
    "            out = x\n",
    "        out = out.view(-1, int(self.feedback_bits / self.B))\n",
    "        out = self.fc(out)\n",
    "        out = out.reshape(-1, 2, 24, 16)\n",
    "        out = self.decoder_feature(out)\n",
    "        out = self.out_cov(out)\n",
    "        out = self.sig(out)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, feedback_bits):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(feedback_bits)\n",
    "        self.decoder = Decoder(feedback_bits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature = self.encoder(x)\n",
    "        out = self.decoder(feature)\n",
    "        return out\n",
    "\n",
    "\n",
    "x = torch.randn(128, 24, 16, 2)\n",
    "model = AutoEncoder(512)\n",
    "g = make_dot(model(x), params=dict(model.named_parameters()))\n",
    "\n",
    "g.render('espnet_model', view=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}