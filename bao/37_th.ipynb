{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    " \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import OrderedDict\n",
    " \n",
    "channelNum = 64\n",
    " \n",
    "class Mish(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x * (torch.tanh(torch.nn.functional.softplus(x)))\n",
    "        return x\n",
    " \n",
    " \n",
    "# This part implement the quantization and dequantization operations.\n",
    "# The output of the encoder must be the bitstream.\n",
    "def Num2Bit(Num, B):\n",
    "    Num_ = Num.type(torch.uint8)\n",
    " \n",
    "    def integer2bit(integer, num_bits=B * 2):\n",
    "        dtype = integer.type()\n",
    "        exponent_bits = -torch.arange(-(num_bits - 1), 1).type(dtype)\n",
    "        exponent_bits = exponent_bits.repeat(integer.shape + (1,))\n",
    "        out = integer.unsqueeze(-1) // 2 ** exponent_bits\n",
    "        return (out - (out % 1)) % 2\n",
    " \n",
    "    bit = integer2bit(Num_)\n",
    "    bit = (bit[:, :, B:]).reshape(-1, Num_.shape[1] * B)\n",
    "    return bit.type(torch.float32)\n",
    " \n",
    "def Bit2Num(Bit, B):\n",
    "    Bit_ = Bit.type(torch.float32)\n",
    "    Bit_ = torch.reshape(Bit_, [-1, int(Bit_.shape[1] / B), B])\n",
    "    num = torch.zeros(Bit_[:, :, 1].shape).cuda()\n",
    "    for i in range(B):\n",
    "        num = num + Bit_[:, :, i] * 2 ** (B - 1 - i)\n",
    "    return num\n",
    " \n",
    "class Quantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = torch.round(x * step - 0.5)\n",
    "        out = Num2Bit(out, B)\n",
    "        return out\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of constant arguments to forward must be None.\n",
    "        # Gradient of a number is the sum of its four bits.\n",
    "        b, _ = grad_output.shape\n",
    "        grad_num = torch.sum(grad_output.reshape(b, -1, ctx.constant), dim=2)\n",
    "        return grad_num, None\n",
    " \n",
    " \n",
    "class Dequantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = Bit2Num(x, B)\n",
    "        out = (out + 0.5) / step\n",
    "        return out\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        # repeat the gradient of a Num for B time.\n",
    "        b, c = grad_output.shape\n",
    "        grad_output = grad_output.unsqueeze(2) / ctx.constant\n",
    "        grad_bit = grad_output.expand(b, c, ctx.constant)\n",
    "        return torch.reshape(grad_bit, (-1, c * ctx.constant)), None\n",
    " \n",
    " \n",
    "class QuantizationLayer(nn.Module):\n",
    " \n",
    "    def __init__(self, B):\n",
    "        super(QuantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = Quantization.apply(x, self.B)\n",
    "        return out\n",
    " \n",
    " \n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequential residual blocks each of which consists of \\\n",
    "    two convolution layers.\n",
    "    Args:\n",
    "        ch (int): number of input and output channels.\n",
    "        nblocks (int): number of residual blocks.\n",
    "        shortcut (bool): if True, residual tensor addition is enabled.\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, ch, nblocks=1, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.shortcut = shortcut\n",
    "        self.module_list = nn.ModuleList()\n",
    "        for i in range(nblocks):\n",
    "            resblock_one = nn.ModuleList()\n",
    "            resblock_one.append(ConvBN(ch, ch, 1))\n",
    "            resblock_one.append(Mish())\n",
    "            resblock_one.append(ConvBN(ch, ch, 3))\n",
    "            resblock_one.append(Mish())\n",
    "            self.module_list.append(resblock_one)\n",
    " \n",
    "    def forward(self, x):\n",
    "        for module in self.module_list:\n",
    "            h = x\n",
    "            for res in module:\n",
    "                h = res(h)\n",
    "            x = x + h if self.shortcut else h\n",
    "        return x\n",
    " \n",
    " \n",
    "class DequantizationLayer(nn.Module):\n",
    " \n",
    "    def __init__(self, B):\n",
    "        super(DequantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = Dequantization.apply(x, self.B)\n",
    "        return out\n",
    " \n",
    " \n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=True)\n",
    " \n",
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, groups=1):\n",
    "        if not isinstance(kernel_size, int):\n",
    "            padding = [(i - 1) // 2 for i in kernel_size]\n",
    "        else:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        super(ConvBN, self).__init__(OrderedDict([\n",
    "            ('conv', nn.Conv2d(in_planes, out_planes, kernel_size, stride,\n",
    "                               padding=padding, groups=groups, bias=False)),\n",
    "            ('bn', nn.BatchNorm2d(out_planes)),\n",
    "            ('Mish', Mish())\n",
    "        ]))\n",
    " \n",
    " \n",
    "class CRBlock64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRBlock64, self).__init__()\n",
    "        self.convbncrb = ConvBN(channelNum, channelNum * 2, 3)\n",
    "        self.path1 = Encoder_conv(channelNum * 2, 4)\n",
    "        self.path2 = nn.Sequential(OrderedDict([\n",
    "            ('conv1x5', ConvBN(channelNum * 2, channelNum * 2, [1, 5])),\n",
    "            ('conv5x1', ConvBN(channelNum * 2, channelNum * 2, [5, 1])),\n",
    "            ('conv5x1', ConvBN(channelNum * 2, channelNum * 2, 1)),\n",
    "            ('conv5x1', ConvBN(channelNum * 2, channelNum * 2, 3)),\n",
    "        ]))\n",
    "        self.encoder_conv = Encoder_conv(channelNum * 4, 4)\n",
    "        self.encoder_conv1 = ConvBN(channelNum * 4, channelNum, 1)\n",
    "        self.identity = nn.Identity()\n",
    "        self.relu = Mish()\n",
    " \n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "        x = self.convbncrb(x)\n",
    "        out1 = self.path1(x)\n",
    "        out2 = self.path2(x)\n",
    "        out = torch.cat((out1, out2), dim=1)\n",
    "        out = self.relu(out)\n",
    "        out = self.encoder_conv(out)\n",
    "        out = self.encoder_conv1(out)\n",
    "        out = self.relu(out + identity)\n",
    "        return out\n",
    " \n",
    " \n",
    "class CRBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRBlock, self).__init__()\n",
    "        self.convban = nn.Sequential(OrderedDict([\n",
    "            (\"conv3x3_bn\", ConvBN(channelNum, channelNum, 3)),\n",
    "        ]))\n",
    "        self.path1 = Encoder_conv(channelNum, 4)\n",
    "        self.path2 = nn.Sequential(OrderedDict([\n",
    "            ('conv1x5', ConvBN(channelNum, channelNum, [1, 5])),\n",
    "            ('conv5x1', ConvBN(channelNum, channelNum, [5, 1])),\n",
    "            (\"conv9x1_bn\", ConvBN(channelNum, channelNum, 1)),\n",
    "        ]))\n",
    "        self.encoder_conv = Encoder_conv(channelNum * 2)\n",
    "        self.encoder_conv1 = ConvBN(channelNum * 2, channelNum, 1)\n",
    "        self.identity = nn.Identity()\n",
    "        self.relu = Mish()\n",
    " \n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "        x = self.convban(x)\n",
    "        out1 = self.path1(x)\n",
    "        out2 = self.path2(x)\n",
    "        out = torch.cat((out1, out2), dim=1)\n",
    "        out = self.relu(out)\n",
    "        out = self.encoder_conv(out)\n",
    "        out = self.encoder_conv1(out)\n",
    "        out = self.relu(out + identity)\n",
    "        return out\n",
    " \n",
    " \n",
    "class Encoder_conv(nn.Module):\n",
    "    def __init__(self, in_planes=128, blocks=2):\n",
    "        super().__init__()\n",
    "        self.conv2 = ConvBN(in_planes, in_planes, [1, 9])\n",
    "        self.conv3 = ConvBN(in_planes, in_planes, [9, 1])\n",
    "        self.conv4 = ConvBN(in_planes, in_planes, 1)\n",
    "        self.resBlock = ResBlock(ch=in_planes, nblocks=blocks)\n",
    "        self.conv5 = ConvBN(in_planes, in_planes, [1, 7])\n",
    "        self.conv6 = ConvBN(in_planes, in_planes, [7, 1])\n",
    "        self.conv7 = ConvBN(in_planes, in_planes, 1)\n",
    "        self.relu = Mish()\n",
    " \n",
    "    def forward(self, input):\n",
    "        x2 = self.conv2(input)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        r1 = self.resBlock(x4)\n",
    "        x5 = self.conv5(r1)\n",
    "        x6 = self.conv6(x5)\n",
    "        x7 = self.conv7(x6)\n",
    "        x7 = self.relu(x7 + x4)\n",
    "        return x7\n",
    " \n",
    " \n",
    "class Encoder(nn.Module):\n",
    "    B = 4\n",
    " \n",
    "    def __init__(self, feedback_bits, quantization=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.convban = nn.Sequential(OrderedDict([\n",
    "            (\"conv3x3_bn\", ConvBN(2, channelNum, 3)),\n",
    "        ]))\n",
    "        self.encoder1 = Encoder_conv(channelNum)\n",
    "        self.encoder2 = nn.Sequential(OrderedDict([\n",
    "            ('conv1x5', ConvBN(channelNum, channelNum, [1, 5])),\n",
    "            ('conv5x1', ConvBN(channelNum, channelNum, [5, 1])),\n",
    "            (\"conv9x1_bn\", ConvBN(channelNum, channelNum, 3)),\n",
    "        ]))\n",
    "        self.encoder_conv = Encoder_conv(channelNum * 2)\n",
    "        self.encoder_conv1 = nn.Sequential(OrderedDict([\n",
    "            (\"conv1x1_bn\", ConvBN(channelNum * 2, 2, 1)),\n",
    "        ]))\n",
    "        self.fc = nn.Linear(1024, int(feedback_bits / self.B))\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.quantize = QuantizationLayer(self.B)\n",
    "        self.quantization = quantization\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.convban(x)\n",
    "        encode1 = self.encoder1(x)\n",
    "        encode2 = self.encoder2(x)\n",
    "        out = torch.cat((encode1, encode2), dim=1)\n",
    "        out = self.encoder_conv(out)\n",
    "        out = self.encoder_conv1(out)\n",
    "        out = out.view(-1, 1024)\n",
    "        out = self.fc(out)\n",
    "        out = self.sig(out)\n",
    "        if self.quantization:\n",
    "            out = self.quantize(out)\n",
    "        else:\n",
    "            out = out\n",
    "        return out\n",
    " \n",
    " \n",
    "class Decoder(nn.Module):\n",
    "    B = 4\n",
    " \n",
    "    def __init__(self, feedback_bits, quantization=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.feedback_bits = feedback_bits\n",
    "        self.dequantize = DequantizationLayer(self.B)\n",
    "        self.fc = nn.Linear(int(feedback_bits / self.B), 1024)\n",
    "        decoder = OrderedDict([\n",
    "            (\"conv3x3_bn\", ConvBN(2, channelNum, 3)),\n",
    "            (\"CRBlock1\", CRBlock64()),\n",
    "            (\"CRBlock2\", CRBlock()),\n",
    "        ])\n",
    "        self.decoder_feature = nn.Sequential(decoder)\n",
    "        self.out_cov = conv3x3(channelNum, 2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.quantization = quantization\n",
    " \n",
    "    def forward(self, x):\n",
    "        if self.quantization:\n",
    "            out = self.dequantize(x)\n",
    "        else:\n",
    "            out = x\n",
    "        out = out.view(-1, int(self.feedback_bits / self.B))\n",
    "        out = self.fc(out)\n",
    "        out = out.view(-1, 2, 16, 32)\n",
    "        out = self.decoder_feature(out)\n",
    "        out = self.out_cov(out)\n",
    "        out = self.sig(out)\n",
    "        return out\n",
    " \n",
    " \n",
    "# Note: Do not modify following class and keep it in your submission.\n",
    "# feedback_bits is 128 by default.\n",
    "class AutoEncoder(nn.Module):\n",
    " \n",
    "    def __init__(self, feedback_bits):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(feedback_bits)\n",
    "        self.decoder = Decoder(feedback_bits)\n",
    " \n",
    "    def forward(self, x):\n",
    "        feature = self.encoder(x)\n",
    "        out = self.decoder(feature)\n",
    "        return out\n",
    " \n",
    " \n",
    "def NMSE(x, x_hat):\n",
    "    x_real = np.reshape(x[:, :, :, 0], (len(x), -1))\n",
    "    x_imag = np.reshape(x[:, :, :, 1], (len(x), -1))\n",
    "    x_hat_real = np.reshape(x_hat[:, :, :, 0], (len(x_hat), -1))\n",
    "    x_hat_imag = np.reshape(x_hat[:, :, :, 1], (len(x_hat), -1))\n",
    "    x_C = x_real - 0.5 + 1j * (x_imag - 0.5)\n",
    "    x_hat_C = x_hat_real - 0.5 + 1j * (x_hat_imag - 0.5)\n",
    "    power = np.sum(abs(x_C) ** 2, axis=1)\n",
    "    mse = np.sum(abs(x_C - x_hat_C) ** 2, axis=1)\n",
    "    nmse = np.mean(mse / power)\n",
    "    return nmse\n",
    " \n",
    "def NMSE_cuda(x, x_hat):\n",
    "    x_real = x[:, 0, :, :].view(len(x), -1) - 0.5\n",
    "    x_imag = x[:, 1, :, :].view(len(x), -1) - 0.5\n",
    "    x_hat_real = x_hat[:, 0, :, :].view(len(x_hat), -1) - 0.5\n",
    "    x_hat_imag = x_hat[:, 1, :, :].view(len(x_hat), -1) - 0.5\n",
    "    power = torch.sum(x_real ** 2 + x_imag ** 2, axis=1)\n",
    "    mse = torch.sum((x_real - x_hat_real) ** 2 + (x_imag - x_hat_imag) ** 2, axis=1)\n",
    "    nmse = mse / power\n",
    "    return nmse\n",
    " \n",
    "class NMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='sum'):\n",
    "        super(NMSELoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    " \n",
    "    def forward(self, x_hat, x):\n",
    "        nmse = NMSE_cuda(x, x_hat)\n",
    "        if self.reduction == 'mean':\n",
    "            nmse = torch.mean(nmse)\n",
    "        else:\n",
    "            nmse = torch.sum(nmse)\n",
    "        return nmse\n",
    " \n",
    " \n",
    "def Score(NMSE):\n",
    "    score = 1 - NMSE\n",
    "    return score\n",
    " \n",
    " \n",
    "# dataLoader\n",
    "class DatasetFolder(Dataset):\n",
    " \n",
    "    def __init__(self, matData):\n",
    "        self.matdata = matData\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.matdata.shape[0]\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        return self.matdata[index]  # , self.matdata[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import OrderedDict\n",
    " \n",
    "channelNum = 64\n",
    " \n",
    "class Mish(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x * (torch.tanh(torch.nn.functional.softplus(x)))\n",
    "        return x\n",
    " \n",
    " \n",
    "# This part implement the quantization and dequantization operations.\n",
    "# The output of the encoder must be the bitstream.\n",
    "def Num2Bit(Num, B):\n",
    "    Num_ = Num.type(torch.uint8)\n",
    " \n",
    "    def integer2bit(integer, num_bits=B * 2):\n",
    "        dtype = integer.type()\n",
    "        exponent_bits = -torch.arange(-(num_bits - 1), 1).type(dtype)\n",
    "        exponent_bits = exponent_bits.repeat(integer.shape + (1,))\n",
    "        out = integer.unsqueeze(-1) // 2 ** exponent_bits\n",
    "        return (out - (out % 1)) % 2\n",
    " \n",
    "    bit = integer2bit(Num_)\n",
    "    bit = (bit[:, :, B:]).reshape(-1, Num_.shape[1] * B)\n",
    "    return bit.type(torch.float32)\n",
    " \n",
    "def Bit2Num(Bit, B):\n",
    "    Bit_ = Bit.type(torch.float32)\n",
    "    Bit_ = torch.reshape(Bit_, [-1, int(Bit_.shape[1] / B), B])\n",
    "    num = torch.zeros(Bit_[:, :, 1].shape).cuda()\n",
    "    for i in range(B):\n",
    "        num = num + Bit_[:, :, i] * 2 ** (B - 1 - i)\n",
    "    return num\n",
    " \n",
    "class Quantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = torch.round(x * step - 0.5)\n",
    "        out = Num2Bit(out, B)\n",
    "        return out\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of constant arguments to forward must be None.\n",
    "        # Gradient of a number is the sum of its four bits.\n",
    "        b, _ = grad_output.shape\n",
    "        grad_num = torch.sum(grad_output.reshape(b, -1, ctx.constant), dim=2)\n",
    "        return grad_num, None\n",
    " \n",
    " \n",
    "class Dequantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = Bit2Num(x, B)\n",
    "        out = (out + 0.5) / step\n",
    "        return out\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        # repeat the gradient of a Num for B time.\n",
    "        b, c = grad_output.shape\n",
    "        grad_output = grad_output.unsqueeze(2) / ctx.constant\n",
    "        grad_bit = grad_output.expand(b, c, ctx.constant)\n",
    "        return torch.reshape(grad_bit, (-1, c * ctx.constant)), None\n",
    " \n",
    " \n",
    "class QuantizationLayer(nn.Module):\n",
    " \n",
    "    def __init__(self, B):\n",
    "        super(QuantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = Quantization.apply(x, self.B)\n",
    "        return out\n",
    " \n",
    " \n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequential residual blocks each of which consists of \\\n",
    "    two convolution layers.\n",
    "    Args:\n",
    "        ch (int): number of input and output channels.\n",
    "        nblocks (int): number of residual blocks.\n",
    "        shortcut (bool): if True, residual tensor addition is enabled.\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, ch, nblocks=1, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.shortcut = shortcut\n",
    "        self.module_list = nn.ModuleList()\n",
    "        for i in range(nblocks):\n",
    "            resblock_one = nn.ModuleList()\n",
    "            resblock_one.append(ConvBN(ch, ch, 1))\n",
    "            resblock_one.append(Mish())\n",
    "            resblock_one.append(ConvBN(ch, ch, 3))\n",
    "            resblock_one.append(Mish())\n",
    "            self.module_list.append(resblock_one)\n",
    " \n",
    "    def forward(self, x):\n",
    "        for module in self.module_list:\n",
    "            h = x\n",
    "            for res in module:\n",
    "                h = res(h)\n",
    "            x = x + h if self.shortcut else h\n",
    "        return x\n",
    " \n",
    " \n",
    "class DequantizationLayer(nn.Module):\n",
    " \n",
    "    def __init__(self, B):\n",
    "        super(DequantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = Dequantization.apply(x, self.B)\n",
    "        return out\n",
    " \n",
    " \n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=True)\n",
    " \n",
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, groups=1):\n",
    "        if not isinstance(kernel_size, int):\n",
    "            padding = [(i - 1) // 2 for i in kernel_size]\n",
    "        else:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        super(ConvBN, self).__init__(OrderedDict([\n",
    "            ('conv', nn.Conv2d(in_planes, out_planes, kernel_size, stride,\n",
    "                               padding=padding, groups=groups, bias=False)),\n",
    "            ('bn', nn.BatchNorm2d(out_planes)),\n",
    "            ('Mish', Mish())\n",
    "        ]))\n",
    " \n",
    " \n",
    "class CRBlock64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRBlock64, self).__init__()\n",
    "        self.convbncrb = ConvBN(channelNum, channelNum * 2, 3)\n",
    "        self.path1 = Encoder_conv(channelNum * 2, 4)\n",
    "        self.path2 = nn.Sequential(OrderedDict([\n",
    "            ('conv1x5', ConvBN(channelNum * 2, channelNum * 2, [1, 5])),\n",
    "            ('conv5x1', ConvBN(channelNum * 2, channelNum * 2, [5, 1])),\n",
    "            ('conv5x1', ConvBN(channelNum * 2, channelNum * 2, 1)),\n",
    "            ('conv5x1', ConvBN(channelNum * 2, channelNum * 2, 3)),\n",
    "        ]))\n",
    "        self.encoder_conv = Encoder_conv(channelNum * 4, 4)\n",
    "        self.encoder_conv1 = ConvBN(channelNum * 4, channelNum, 1)\n",
    "        self.identity = nn.Identity()\n",
    "        self.relu = Mish()\n",
    " \n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "        x = self.convbncrb(x)\n",
    "        out1 = self.path1(x)\n",
    "        out2 = self.path2(x)\n",
    "        out = torch.cat((out1, out2), dim=1)\n",
    "        out = self.relu(out)\n",
    "        out = self.encoder_conv(out)\n",
    "        out = self.encoder_conv1(out)\n",
    "        out = self.relu(out + identity)\n",
    "        return out\n",
    " \n",
    " \n",
    "class CRBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRBlock, self).__init__()\n",
    "        self.convban = nn.Sequential(OrderedDict([\n",
    "            (\"conv3x3_bn\", ConvBN(channelNum, channelNum, 3)),\n",
    "        ]))\n",
    "        self.path1 = Encoder_conv(channelNum, 4)\n",
    "        self.path2 = nn.Sequential(OrderedDict([\n",
    "            ('conv1x5', ConvBN(channelNum, channelNum, [1, 5])),\n",
    "            ('conv5x1', ConvBN(channelNum, channelNum, [5, 1])),\n",
    "            (\"conv9x1_bn\", ConvBN(channelNum, channelNum, 1)),\n",
    "        ]))\n",
    "        self.encoder_conv = Encoder_conv(channelNum * 2)\n",
    "        self.encoder_conv1 = ConvBN(channelNum * 2, channelNum, 1)\n",
    "        self.identity = nn.Identity()\n",
    "        self.relu = Mish()\n",
    " \n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "        x = self.convban(x)\n",
    "        out1 = self.path1(x)\n",
    "        out2 = self.path2(x)\n",
    "        out = torch.cat((out1, out2), dim=1)\n",
    "        out = self.relu(out)\n",
    "        out = self.encoder_conv(out)\n",
    "        out = self.encoder_conv1(out)\n",
    "        out = self.relu(out + identity)\n",
    "        return out\n",
    " \n",
    " \n",
    "class Encoder_conv(nn.Module):\n",
    "    def __init__(self, in_planes=128, blocks=2):\n",
    "        super().__init__()\n",
    "        self.conv2 = ConvBN(in_planes, in_planes, [1, 9])\n",
    "        self.conv3 = ConvBN(in_planes, in_planes, [9, 1])\n",
    "        self.conv4 = ConvBN(in_planes, in_planes, 1)\n",
    "        self.resBlock = ResBlock(ch=in_planes, nblocks=blocks)\n",
    "        self.conv5 = ConvBN(in_planes, in_planes, [1, 7])\n",
    "        self.conv6 = ConvBN(in_planes, in_planes, [7, 1])\n",
    "        self.conv7 = ConvBN(in_planes, in_planes, 1)\n",
    "        self.relu = Mish()\n",
    " \n",
    "    def forward(self, input):\n",
    "        x2 = self.conv2(input)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        r1 = self.resBlock(x4)\n",
    "        x5 = self.conv5(r1)\n",
    "        x6 = self.conv6(x5)\n",
    "        x7 = self.conv7(x6)\n",
    "        x7 = self.relu(x7 + x4)\n",
    "        return x7\n",
    " \n",
    " \n",
    "class Encoder(nn.Module):\n",
    "    B = 4\n",
    " \n",
    "    def __init__(self, feedback_bits, quantization=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.convban = nn.Sequential(OrderedDict([\n",
    "            (\"conv3x3_bn\", ConvBN(2, channelNum, 3)),\n",
    "        ]))\n",
    "        self.encoder1 = Encoder_conv(channelNum)\n",
    "        self.encoder2 = nn.Sequential(OrderedDict([\n",
    "            ('conv1x5', ConvBN(channelNum, channelNum, [1, 5])),\n",
    "            ('conv5x1', ConvBN(channelNum, channelNum, [5, 1])),\n",
    "            (\"conv9x1_bn\", ConvBN(channelNum, channelNum, 3)),\n",
    "        ]))\n",
    "        self.encoder_conv = Encoder_conv(channelNum * 2)\n",
    "        self.encoder_conv1 = nn.Sequential(OrderedDict([\n",
    "            (\"conv1x1_bn\", ConvBN(channelNum * 2, 2, 1)),\n",
    "        ]))\n",
    "        self.fc = nn.Linear(1024, int(feedback_bits / self.B))\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.quantize = QuantizationLayer(self.B)\n",
    "        self.quantization = quantization\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.convban(x)\n",
    "        encode1 = self.encoder1(x)\n",
    "        encode2 = self.encoder2(x)\n",
    "        out = torch.cat((encode1, encode2), dim=1)\n",
    "        out = self.encoder_conv(out)\n",
    "        out = self.encoder_conv1(out)\n",
    "        out = out.view(-1, 1024)\n",
    "        out = self.fc(out)\n",
    "        out = self.sig(out)\n",
    "        if self.quantization:\n",
    "            out = self.quantize(out)\n",
    "        else:\n",
    "            out = out\n",
    "        return out\n",
    " \n",
    " \n",
    "class Decoder(nn.Module):\n",
    "    B = 4\n",
    " \n",
    "    def __init__(self, feedback_bits, quantization=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.feedback_bits = feedback_bits\n",
    "        self.dequantize = DequantizationLayer(self.B)\n",
    "        self.fc = nn.Linear(int(feedback_bits / self.B), 1024)\n",
    "        decoder = OrderedDict([\n",
    "            (\"conv3x3_bn\", ConvBN(2, channelNum, 3)),\n",
    "            (\"CRBlock1\", CRBlock64()),\n",
    "            (\"CRBlock2\", CRBlock()),\n",
    "        ])\n",
    "        self.decoder_feature = nn.Sequential(decoder)\n",
    "        self.out_cov = conv3x3(channelNum, 2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.quantization = quantization\n",
    " \n",
    "    def forward(self, x):\n",
    "        if self.quantization:\n",
    "            out = self.dequantize(x)\n",
    "        else:\n",
    "            out = x\n",
    "        out = out.view(-1, int(self.feedback_bits / self.B))\n",
    "        out = self.fc(out)\n",
    "        out = out.view(-1, 2, 16, 32)\n",
    "        out = self.decoder_feature(out)\n",
    "        out = self.out_cov(out)\n",
    "        out = self.sig(out)\n",
    "        return out\n",
    " \n",
    " \n",
    "# Note: Do not modify following class and keep it in your submission.\n",
    "# feedback_bits is 128 by default.\n",
    "class AutoEncoder(nn.Module):\n",
    " \n",
    "    def __init__(self, feedback_bits):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(feedback_bits)\n",
    "        self.decoder = Decoder(feedback_bits)\n",
    " \n",
    "    def forward(self, x):\n",
    "        feature = self.encoder(x)\n",
    "        out = self.decoder(feature)\n",
    "        return out\n",
    " \n",
    " \n",
    "def NMSE(x, x_hat):\n",
    "    x_real = np.reshape(x[:, :, :, 0], (len(x), -1))\n",
    "    x_imag = np.reshape(x[:, :, :, 1], (len(x), -1))\n",
    "    x_hat_real = np.reshape(x_hat[:, :, :, 0], (len(x_hat), -1))\n",
    "    x_hat_imag = np.reshape(x_hat[:, :, :, 1], (len(x_hat), -1))\n",
    "    x_C = x_real - 0.5 + 1j * (x_imag - 0.5)\n",
    "    x_hat_C = x_hat_real - 0.5 + 1j * (x_hat_imag - 0.5)\n",
    "    power = np.sum(abs(x_C) ** 2, axis=1)\n",
    "    mse = np.sum(abs(x_C - x_hat_C) ** 2, axis=1)\n",
    "    nmse = np.mean(mse / power)\n",
    "    return nmse\n",
    " \n",
    "def NMSE_cuda(x, x_hat):\n",
    "    x_real = x[:, 0, :, :].view(len(x), -1) - 0.5\n",
    "    x_imag = x[:, 1, :, :].view(len(x), -1) - 0.5\n",
    "    x_hat_real = x_hat[:, 0, :, :].view(len(x_hat), -1) - 0.5\n",
    "    x_hat_imag = x_hat[:, 1, :, :].view(len(x_hat), -1) - 0.5\n",
    "    power = torch.sum(x_real ** 2 + x_imag ** 2, axis=1)\n",
    "    mse = torch.sum((x_real - x_hat_real) ** 2 + (x_imag - x_hat_imag) ** 2, axis=1)\n",
    "    nmse = mse / power\n",
    "    return nmse\n",
    " \n",
    "class NMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='sum'):\n",
    "        super(NMSELoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    " \n",
    "    def forward(self, x_hat, x):\n",
    "        nmse = NMSE_cuda(x, x_hat)\n",
    "        if self.reduction == 'mean':\n",
    "            nmse = torch.mean(nmse)\n",
    "        else:\n",
    "            nmse = torch.sum(nmse)\n",
    "        return nmse\n",
    " \n",
    " \n",
    "def Score(NMSE):\n",
    "    score = 1 - NMSE\n",
    "    return score\n",
    " \n",
    " \n",
    "# dataLoader\n",
    "class DatasetFolder(Dataset):\n",
    " \n",
    "    def __init__(self, matData):\n",
    "        self.matdata = matData\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.matdata.shape[0]\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        return self.matdata[index]  # , self.matdata[index]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
